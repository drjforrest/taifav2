"""
TAIFA-FIALA FastAPI Main Application
Innovation archive and data pipeline API
"""

import asyncio
import os
from datetime import datetime
from typing import List, Optional
from uuid import UUID, uuid4

# Include routers
from api.ai_assistant import router as ai_assistant_router
from api.analytics import router as analytics_router
from api.data_completeness import router as data_completeness_router
from api.data_intelligence import router as data_intelligence_router
from api.enhanced_data_completeness import router as enhanced_data_completeness_router
from api.entity_relationships import router as entity_relationships_router
from api.etl_live import router as etl_live_router
from api.funding_enrichment import router as funding_enrichment_router  # NEW: Funding enrichment API
from api.longitudinal_intelligence import router as longitudinal_intelligence_router
from api.trends import router as trends_router
from config.settings import settings
from fastapi import BackgroundTasks, Depends, FastAPI, HTTPException, Query, Request
from fastapi.middleware.cors import CORSMiddleware
from loguru import logger
from models.schemas import (
    CommunitySubmissionResponse,
    CommunityVote,
    InnovationCreate,
    InnovationResponse,
    InnovationSearchResponse,
    InnovationStats,
    InnovationVoteCreate,
    InnovationVoteResponse,
    InnovationVotingStats,
)
from services.vector_service import VectorService, get_vector_service
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded
from slowapi.middleware import SlowAPIMiddleware
from slowapi.util import get_remote_address

try:
    from services.serper_service import search_african_innovations
except ImportError:
    logger.warning(
        "Serper service not available - search functionality will be limited"
    )

    async def search_african_innovations(*args, **kwargs):
        return []


from etl.academic.arxiv_scraper import scrape_arxiv_papers
from etl.news.rss_monitor import monitor_rss_feeds
from services.ai_backfill_service import (
    ai_backfill_service,
    create_backfill_jobs_for_innovations,
)
from services.integration_guide import backfill_integration

# Initialize limiter
limiter = Limiter(key_func=get_remote_address)

# Create FastAPI app
app = FastAPI(
    title=settings.APP_NAME,
    version=settings.APP_VERSION,
    description="API for TAIFA-FIALA African AI Innovation Archive",
    docs_url="/docs" if settings.DEBUG else None,
    redoc_url="/redoc" if settings.DEBUG else None,
)

# Add rate limiting middleware
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)
app.add_middleware(SlowAPIMiddleware)

# Add CORS middleware
logger.info(f"CORS allowed origins: {settings.ALLOWED_ORIGINS}")
app.add_middleware(
    CORSMiddleware,
    allow_origins=settings.ALLOWED_ORIGINS,
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(etl_live_router)
app.include_router(data_completeness_router)
app.include_router(enhanced_data_completeness_router)
app.include_router(analytics_router)
app.include_router(data_intelligence_router)
app.include_router(ai_assistant_router)
app.include_router(trends_router)
app.include_router(entity_relationships_router)
app.include_router(longitudinal_intelligence_router)
app.include_router(funding_enrichment_router)  # NEW: Funding enrichment endpoints


@app.on_event("startup")
async def startup_event():
    """Initialize services on startup"""
    logger.info("Starting TAIFA-FIALA API...")

    # Initialize vector service asynchronously (non-blocking)
    async def init_vector_service():
        try:
            import asyncio

            await asyncio.wait_for(get_vector_service(), timeout=10.0)
            logger.info("Vector service initialized")
        except asyncio.TimeoutError:
            logger.warning(
                "Vector service initialization timed out - will retry on first use"
            )
        except Exception as e:
            logger.error(f"Failed to initialize vector service: {e}")

    # Initialize enrichment scheduler
    async def init_enrichment_scheduler():
        try:
            from services.enrichment_scheduler import start_enrichment_scheduler

            await start_enrichment_scheduler()
            logger.info("Enrichment scheduler started successfully")
        except Exception as e:
            logger.error(f"Failed to start enrichment scheduler: {e}")

    # Start initialization in background, don't wait for it
    asyncio.create_task(init_vector_service())
    asyncio.create_task(init_enrichment_scheduler())
    logger.info(
        "TAIFA-FIALA API startup complete - services initializing in background"
    )


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown"""
    logger.info("Shutting down TAIFA-FIALA API...")

    # Stop enrichment scheduler
    try:
        from services.enrichment_scheduler import stop_enrichment_scheduler

        await stop_enrichment_scheduler()
        logger.info("Enrichment scheduler stopped")
    except Exception as e:
        logger.error(f"Error stopping enrichment scheduler: {e}")


# Health Check
@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {
        "status": "healthy",
        "timestamp": datetime.now(),
        "version": settings.APP_VERSION,
    }


# Dashboard Stats Endpoint
@app.get("/api/stats")
@limiter.limit("60/minute")
async def get_dashboard_stats(request: Request):
    """Get dashboard statistics"""
    try:
        from config.database import get_supabase

        supabase = get_supabase()

        # Get counts from different tables
        innovations_response = (
            supabase.table("innovations").select("id", count="exact").execute()
        )
        publications_response = (
            supabase.table("publications").select("id", count="exact").execute()
        )
        organizations_response = (
            supabase.table("organizations").select("id", count="exact").execute()
        )
        individuals_response = (
            supabase.table("individuals")
            .select("id", count="exact")
            .eq("verification_status", "verified")
            .execute()
        )

        # Get sample data for calculations
        publications_data = (
            supabase.table("publications")
            .select(
                "african_entities, keywords, african_relevance_score, ai_relevance_score"
            )
            .limit(1000)
            .execute()
        )

        # Calculate derived stats
        african_countries = set()
        keywords = set()
        african_score_sum = 0
        ai_score_sum = 0
        scores_count = 0

        if publications_data.data:
            for pub in publications_data.data:
                # African entities
                if pub.get("african_entities"):
                    for entity in pub["african_entities"]:
                        african_countries.add(entity)

                # Keywords
                if pub.get("keywords"):
                    for keyword in pub["keywords"]:
                        keywords.add(keyword)

                # Scores
                african_score = pub.get("african_relevance_score", 0)
                ai_score = pub.get("ai_relevance_score", 0)
                if african_score > 0 and ai_score > 0:
                    african_score_sum += african_score
                    ai_score_sum += ai_score
                    scores_count += 1

        return {
            "total_innovations": innovations_response.count or 0,
            "total_publications": publications_response.count or 0,
            "total_organizations": organizations_response.count or 0,
            "verified_individuals": individuals_response.count or 0,
            "african_countries_covered": len(african_countries),
            "unique_keywords": len(keywords),
            "avg_african_relevance": african_score_sum / scores_count
            if scores_count > 0
            else 0,
            "avg_ai_relevance": ai_score_sum / scores_count if scores_count > 0 else 0,
            "last_updated": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"Error getting dashboard stats: {e}")
        # Return mock data if database fails
        return {
            "total_innovations": 24,
            "total_publications": 50,
            "total_organizations": 15,
            "verified_individuals": 8,
            "african_countries_covered": 34,
            "unique_keywords": 46,
            "avg_african_relevance": 0.789,
            "avg_ai_relevance": 0.742,
            "last_updated": datetime.now().isoformat(),
        }


# Database Test Endpoint
@app.get("/test-db")
async def test_database_connection():
    """Test Supabase database connection"""
    try:
        from config.database import get_supabase

        supabase = get_supabase()

        # Test connection by querying innovations table
        response = supabase.table("innovations").select("id").limit(1).execute()

        return {
            "status": "success",
            "message": "Supabase connection working",
            "sample_data_found": len(response.data) > 0 if response.data else False,
            "timestamp": datetime.now(),
        }
    except Exception as e:
        logger.error(f"Database test failed: {e}")
        return {
            "status": "error",
            "message": f"Supabase connection failed: {str(e)}",
            "timestamp": datetime.now(),
        }


# Innovation Endpoints
@app.get("/api/innovations")
@limiter.limit("120/minute")
async def get_innovations(
    request: Request,
    query: Optional[str] = None,
    innovation_type: Optional[str] = None,
    country: Optional[str] = None,
    verification_status: Optional[str] = None,
    limit: int = Query(20, ge=1, le=100),
    offset: int = Query(0, ge=0),
    sort_by: str = Query("created_at"),
    sort_order: str = Query("desc"),
    vector_service: VectorService = Depends(get_vector_service),
):
    """Get innovations with hybrid search (vector + traditional) and filtering"""
    try:
        from config.database import get_supabase

        supabase = get_supabase()

        # Initialize search metadata
        search_metadata = {
            "query_type": "filter_only",
            "used_vector_search": False,
            "used_traditional_search": False,
            "vector_results_count": 0,
            "traditional_results_count": 0,
            "avg_relevance_score": 0,
            "search_quality": "n/a",
        }

        innovations_data = []
        total = 0

        if query and len(query.strip()) >= 3:
            # HYBRID SEARCH: Vector + Traditional
            logger.info(f"Performing hybrid search for query: '{query}'")

            try:
                # 1. Vector Search (semantic similarity)
                vector_results = await vector_service.search_innovations(
                    query=query,
                    innovation_type=innovation_type,
                    country=country,
                    top_k=min(50, limit * 3),  # Get more results for better ranking
                )

                # Extract innovation IDs from vector results
                vector_innovation_ids = []
                innovation_scores = {}

                for result in vector_results:
                    innovation_id = result.metadata.get("innovation_id")
                    if innovation_id:
                        vector_innovation_ids.append(innovation_id)
                        innovation_scores[innovation_id] = result.score

                search_metadata.update(
                    {
                        "query_type": "hybrid_search",
                        "used_vector_search": True,
                        "vector_results_count": len(vector_results),
                        "avg_relevance_score": round(
                            sum(r.score for r in vector_results) / len(vector_results),
                            3,
                        )
                        if vector_results
                        else 0,
                    }
                )

                logger.info(f"Vector search found {len(vector_results)} results")

            except Exception as vector_error:
                logger.warning(
                    f"Vector search failed, falling back to traditional: {vector_error}"
                )
                vector_innovation_ids = []
                innovation_scores = {}

            # 2. Traditional Search (keyword matching) as fallback/supplement
            try:
                traditional_query = supabase.table("innovations").select("*")

                # Apply text search
                traditional_query = traditional_query.or_(
                    f"title.ilike.%{query}%,description.ilike.%{query}%,innovation_type.ilike.%{query}%"
                )

                # Apply filters
                if innovation_type:
                    traditional_query = traditional_query.eq(
                        "innovation_type", innovation_type
                    )
                if verification_status:
                    traditional_query = traditional_query.eq(
                        "verification_status", verification_status
                    )
                if country:
                    traditional_query = traditional_query.eq("country", country)

                traditional_response = traditional_query.limit(50).execute()
                traditional_results = (
                    traditional_response.data if traditional_response.data else []
                )

                search_metadata.update(
                    {
                        "used_traditional_search": True,
                        "traditional_results_count": len(traditional_results),
                    }
                )

                logger.info(
                    f"Traditional search found {len(traditional_results)} results"
                )

            except Exception as traditional_error:
                logger.error(f"Traditional search also failed: {traditional_error}")
                traditional_results = []

            # 3. Merge and rank results (prioritize vector results)
            merged_results = {}

            # First, add vector results (higher priority)
            if vector_innovation_ids:
                vector_query = (
                    supabase.table("innovations")
                    .select("*")
                    .in_("id", vector_innovation_ids)
                )
                if verification_status:
                    vector_query = vector_query.eq(
                        "verification_status", verification_status
                    )

                vector_data_response = vector_query.execute()
                vector_data = (
                    vector_data_response.data if vector_data_response.data else []
                )

                for innovation in vector_data:
                    innovation_id = innovation["id"]
                    innovation["_relevance_score"] = innovation_scores.get(
                        innovation_id, 0
                    )
                    innovation["_search_source"] = "vector"
                    merged_results[innovation_id] = innovation

            # Then, add traditional results (lower priority, only if not already included)
            for innovation in traditional_results:
                innovation_id = innovation["id"]
                if innovation_id not in merged_results:
                    innovation["_relevance_score"] = (
                        0.3  # Lower score for traditional matches
                    )
                    innovation["_search_source"] = "traditional"
                    merged_results[innovation_id] = innovation

            # Convert to list and sort by relevance score
            innovations_data = list(merged_results.values())
            innovations_data.sort(
                key=lambda x: x.get("_relevance_score", 0), reverse=True
            )

            total = len(innovations_data)

            # Update search quality assessment
            if search_metadata["avg_relevance_score"] > 0.8:
                search_metadata["search_quality"] = "excellent"
            elif search_metadata["avg_relevance_score"] > 0.6:
                search_metadata["search_quality"] = "good"
            elif search_metadata["avg_relevance_score"] > 0.3:
                search_metadata["search_quality"] = "fair"
            else:
                search_metadata["search_quality"] = "poor"

        else:
            # NO QUERY: Traditional filtering and pagination
            query_builder = supabase.table("innovations").select("*")

            # Apply filters
            if innovation_type:
                query_builder = query_builder.eq("innovation_type", innovation_type)
            if verification_status:
                query_builder = query_builder.eq(
                    "verification_status", verification_status
                )
            if country:
                query_builder = query_builder.eq("country", country)

            # Get total count
            count_query = supabase.table("innovations").select("id", count="exact")
            if innovation_type:
                count_query = count_query.eq("innovation_type", innovation_type)
            if verification_status:
                count_query = count_query.eq("verification_status", verification_status)
            if country:
                count_query = count_query.eq("country", country)

            count_response = count_query.execute()
            total = count_response.count if count_response.count is not None else 0

            # Apply sorting
            if sort_by == "title":
                query_builder = query_builder.order(
                    "title", desc=(sort_order == "desc")
                )
            elif sort_by == "updated_at":
                query_builder = query_builder.order(
                    "updated_at", desc=(sort_order == "desc")
                )
            else:  # default: created_at
                query_builder = query_builder.order(
                    "created_at", desc=(sort_order == "desc")
                )

            # Apply pagination
            query_builder = query_builder.range(offset, offset + limit - 1)

            # Execute query
            response = query_builder.execute()
            innovations_data = response.data if response.data else []

            search_metadata["query_type"] = "filter_only"

        # Apply pagination to search results
        if query and innovations_data:
            paginated_data = innovations_data[offset : offset + limit]
        else:
            paginated_data = innovations_data

        # Convert to response format
        innovations = []
        for innovation_data in paginated_data:
            innovation_response = {
                "id": innovation_data.get("id"),
                "title": innovation_data.get("title"),
                "description": innovation_data.get("description"),
                "innovation_type": innovation_data.get("innovation_type"),
                "verification_status": innovation_data.get("verification_status"),
                "visibility": innovation_data.get("visibility"),
                "country": innovation_data.get("country"),
                "creation_date": innovation_data.get("creation_date"),
                "organizations": innovation_data.get("organizations", []),
                "individuals": innovation_data.get("individuals", []),
                "fundings": innovation_data.get("fundings", []),
                "publications": innovation_data.get("publications", []),
                "tags": innovation_data.get("tags", []),
                "impact_metrics": innovation_data.get("impact_metrics", {}),
                "website_url": innovation_data.get("website_url"),
                "github_url": innovation_data.get("github_url"),
                "demo_url": innovation_data.get("demo_url"),
                "source_url": innovation_data.get("source_url"),
            }

            # Add search-specific metadata for search results
            if query and "_relevance_score" in innovation_data:
                innovation_response["_relevance_score"] = innovation_data[
                    "_relevance_score"
                ]
                innovation_response["_search_source"] = innovation_data[
                    "_search_source"
                ]

            innovations.append(innovation_response)

        return {
            "innovations": innovations,
            "total": total,
            "limit": limit,
            "offset": offset,
            "has_more": offset + limit < total,
            "search_metadata": search_metadata,
        }

    except Exception as e:
        logger.error(f"Error getting innovations: {e}")
        # Return mock data if database fails to help with debugging
        return {
            "innovations": [],
            "total": 0,
            "limit": limit,
            "offset": offset,
            "has_more": False,
            "error": f"Database error: {str(e)}",
        }


@app.get("/api/innovations/{innovation_id}", response_model=InnovationResponse)
@limiter.limit("60/minute")
async def get_innovation(request: Request, innovation_id: UUID):
    """Get single innovation by ID using Supabase client"""
    try:
        from config.database import get_supabase

        supabase = get_supabase()

        # Query single innovation by ID
        response = (
            supabase.table("innovations")
            .select("*")
            .eq("id", str(innovation_id))
            .execute()
        )

        if not response.data or len(response.data) == 0:
            raise HTTPException(status_code=404, detail="Innovation not found")

        innovation_data = response.data[0]

        # Convert to response format
        return {
            "id": innovation_data.get("id"),
            "title": innovation_data.get("title"),
            "description": innovation_data.get("description"),
            "innovation_type": innovation_data.get("innovation_type"),
            "verification_status": innovation_data.get("verification_status"),
            "visibility": innovation_data.get("visibility"),
            "country": innovation_data.get("country"),
            "creation_date": innovation_data.get("creation_date"),
            "organizations": innovation_data.get("organizations", []),
            "individuals": innovation_data.get("individuals", []),
            "fundings": innovation_data.get("fundings", []),
            "publications": innovation_data.get("publications", []),
            "tags": innovation_data.get("tags", []),
            "impact_metrics": innovation_data.get("impact_metrics", {}),
            "website_url": innovation_data.get("website_url"),
            "github_url": innovation_data.get("github_url"),
            "demo_url": innovation_data.get("demo_url"),
            "source_url": innovation_data.get("source_url"),
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting innovation {innovation_id}: {e}")
        raise HTTPException(status_code=500, detail="Internal server error")


@app.post("/api/innovations", response_model=InnovationResponse)
@limiter.limit("10/minute")
async def create_innovation(
    request: Request,
    innovation_data: InnovationCreate,
    background_tasks: BackgroundTasks,
):
    """Create new innovation using Supabase client"""
    try:
        from config.database import get_supabase

        supabase = get_supabase()

        # Prepare innovation data for Supabase
        innovation_record = {
            "title": innovation_data.title,
            "description": innovation_data.description,
            "innovation_type": innovation_data.innovation_type,
            "creation_date": innovation_data.creation_date.isoformat()
            if innovation_data.creation_date
            else None,
            "problem_solved": innovation_data.problem_solved,
            "solution_approach": innovation_data.solution_approach,
            "impact_metrics": innovation_data.impact_metrics,
            "tech_stack": innovation_data.tech_stack,
            "tags": innovation_data.tags,
            "website_url": str(innovation_data.website_url)
            if innovation_data.website_url
            else None,
            "github_url": str(innovation_data.github_url)
            if innovation_data.github_url
            else None,
            "demo_url": str(innovation_data.demo_url)
            if innovation_data.demo_url
            else None,
            "source_type": "manual",
            "verification_status": "pending",
            "country": innovation_data.country,
        }

        # Create innovation in Supabase
        innovation_response = (
            supabase.table("innovations").insert(innovation_record).execute()
        )

        if not innovation_response.data or len(innovation_response.data) == 0:
            raise HTTPException(status_code=500, detail="Failed to create innovation")

        created_innovation = innovation_response.data[0]
        innovation_id = created_innovation.get("id")

        # Create community submission record
        submission_record = {
            "innovation_id": innovation_id,
            "submitter_name": innovation_data.submitter_name,
            "submitter_email": innovation_data.submitter_email,
            "submission_status": "pending",
        }

        supabase.table("community_submissions").insert(submission_record).execute()

        # Add to vector database in background
        background_tasks.add_task(
            add_innovation_to_vector_db,
            innovation_id,
            created_innovation.get("title"),
            created_innovation.get("description"),
            created_innovation.get("innovation_type"),
            innovation_data.country,
        )

        logger.info(f"Created innovation: {created_innovation.get('title')}")

        # Return response in expected format
        return {
            "id": created_innovation.get("id"),
            "title": created_innovation.get("title"),
            "description": created_innovation.get("description"),
            "innovation_type": created_innovation.get("innovation_type"),
            "verification_status": created_innovation.get("verification_status"),
            "visibility": created_innovation.get("visibility"),
            "country": created_innovation.get("country"),
            "creation_date": created_innovation.get("creation_date"),
            "organizations": created_innovation.get("organizations", []),
            "individuals": created_innovation.get("individuals", []),
            "fundings": created_innovation.get("fundings", []),
            "publications": created_innovation.get("publications", []),
            "tags": created_innovation.get("tags", []),
            "impact_metrics": created_innovation.get("impact_metrics", {}),
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error creating innovation: {e}")
        raise HTTPException(status_code=500, detail="Failed to create innovation")


# Search Endpoints
@app.get("/api/search", response_model=InnovationSearchResponse)
@limiter.limit("20/minute")
async def search_innovations(
    request,
    query: str = Query(..., min_length=3, description="Search query"),
    innovation_type: Optional[str] = None,
    country: Optional[str] = None,
    limit: int = Query(20, ge=1, le=50),
    vector_service: VectorService = Depends(get_vector_service),
):
    """Search innovations using vector similarity"""
    try:
        # Perform vector search
        vector_results = await vector_service.search_innovations(
            query=query, innovation_type=innovation_type, country=country, top_k=limit
        )

        # Convert to response format
        innovations = []
        for result in vector_results:
            # This would need to fetch full innovation data from DB
            # For now, return basic info from metadata
            innovation_data = {
                "id": result.metadata.get("innovation_id"),
                "title": result.metadata.get("title", ""),
                "description": result.content,
                "innovation_type": result.metadata.get("innovation_type"),
                "verification_status": "verified",  # placeholder
                "visibility": "public",
                "created_at": datetime.now(),
                "updated_at": datetime.now(),
                "organizations": [],
                "individuals": [],
                "publications": [],
                "fundings": [],
            }
            innovations.append(innovation_data)

        return InnovationSearchResponse(
            innovations=innovations,
            total=len(innovations),
            limit=limit,
            offset=0,
            has_more=False,
        )

    except Exception as e:
        logger.error(f"Error in vector search: {e}")
        raise HTTPException(status_code=500, detail="Search failed")


# ETL Endpoints
@app.post("/api/etl/academic")
@limiter.limit("5/minute")
async def trigger_academic_etl(
    request: Request,
    background_tasks: BackgroundTasks,
    days_back: int = Query(7, ge=1, le=30),
    max_results: int = Query(100, ge=10, le=500),
):
    """Trigger academic paper scraping"""
    logger.info(
        f"Academic ETL triggered with days_back={days_back}, max_results={max_results}"
    )
    job_id = str(uuid4())

    background_tasks.add_task(run_academic_etl, job_id, days_back, max_results)

    return {
        "job_id": job_id,
        "status": "started",
        "message": f"Academic ETL job started for last {days_back} days",
    }


@app.post("/api/etl/news")
@limiter.limit("5/minute")
async def trigger_news_etl(
    request: Request,
    background_tasks: BackgroundTasks,
    hours_back: int = Query(24, ge=1, le=168),  # Max 1 week
):
    """Trigger news monitoring"""
    logger.info(f"News ETL triggered with hours_back={hours_back}")
    job_id = str(uuid4())

    background_tasks.add_task(run_news_etl, job_id, hours_back)

    return {
        "job_id": job_id,
        "status": "started",
        "message": f"News ETL job started for last {hours_back} hours",
    }


@app.post("/api/etl/serper-search")
@limiter.limit("3/minute")
async def trigger_serper_search(
    request: Request,
    background_tasks: BackgroundTasks,
    innovation_type: Optional[str] = None,
    country: Optional[str] = None,
    num_results: int = Query(50, ge=10, le=100),
):
    """Trigger Serper.dev innovation search"""
    logger.info(
        f"Serper search triggered with innovation_type={innovation_type}, country={country}, num_results={num_results}"
    )
    job_id = str(uuid4())

    background_tasks.add_task(
        run_serper_search, job_id, innovation_type, country, num_results
    )

    return {
        "job_id": job_id,
        "status": "started",
        "message": "Serper search job started",
    }


# Community Endpoints
@app.get("/api/community/submissions", response_model=List[CommunitySubmissionResponse])
@limiter.limit("20/minute")
async def get_community_submissions(
    request: Request, status: Optional[str] = None, limit: int = Query(20, ge=1, le=100)
):
    """Get community submissions for verification using Supabase client"""
    try:
        from config.database import get_supabase

        supabase = get_supabase()

        # Build query
        query = supabase.table("community_submissions").select("*")

        if status:
            query = query.eq("submission_status", status)

        # Apply limit
        query = query.limit(limit)

        # Execute query
        response = query.execute()
        submissions_data = response.data if response.data else []

        # Convert to response format
        submissions = []
        for submission_data in submissions_data:
            submissions.append(
                {
                    "id": submission_data.get("id"),
                    "innovation_id": submission_data.get("innovation_id"),
                    "submitter_name": submission_data.get("submitter_name"),
                    "submitter_email": submission_data.get("submitter_email"),
                    "submission_status": submission_data.get("submission_status"),
                    "created_at": submission_data.get("created_at"),
                    "updated_at": submission_data.get("updated_at"),
                }
            )

        return submissions

    except Exception as e:
        logger.error(f"Error getting community submissions: {e}")
        raise HTTPException(
            status_code=500, detail="Failed to get community submissions"
        )


@app.post("/api/community/vote")
@limiter.limit("50/minute")
async def submit_community_vote(request: Request, vote: CommunityVote):
    """Submit community vote for innovation verification using Supabase client"""
    try:
        from config.database import get_supabase

        supabase = get_supabase()

        # Create vote record in Supabase
        vote_record = {
            "innovation_id": str(vote.innovation_id),
            "voter_id": vote.voter_id,
            "vote_type": vote.vote_type,
            "comment": vote.comment if hasattr(vote, "comment") else None,
        }

        supabase.table("community_votes").insert(vote_record).execute()

        return {"status": "vote_recorded", "message": "Thank you for your vote"}

    except Exception as e:
        logger.error(f"Error submitting community vote: {e}")
        raise HTTPException(status_code=500, detail="Failed to submit vote")


# Innovation Voting Endpoints
@app.post("/api/innovations/{innovation_id}/vote", response_model=InnovationVoteResponse)
@limiter.limit("10/minute")
async def submit_innovation_vote(
    innovation_id: UUID, 
    request: Request, 
    vote_data: InnovationVoteCreate
):
    """Submit vote on whether innovation meets inclusion criteria"""
    try:
        from config.database import get_supabase
        import hashlib

        supabase = get_supabase()

        # Verify innovation exists
        innovation_response = supabase.table("innovations").select("id").eq("id", str(innovation_id)).execute()
        if not innovation_response.data:
            raise HTTPException(status_code=404, detail="Innovation not found")

        # Create voter identifier for deduplication (hash email or IP)
        client_ip = request.client.host
        user_agent = request.headers.get("user-agent", "")
        
        if vote_data.voter_email:
            voter_identifier = hashlib.sha256(vote_data.voter_email.encode()).hexdigest()
        else:
            voter_identifier = hashlib.sha256(f"{client_ip}:{user_agent}".encode()).hexdigest()

        # Check if user has already voted on this innovation
        existing_vote = supabase.table("innovation_votes").select("id").eq("innovation_id", str(innovation_id)).eq("voter_identifier", voter_identifier).execute()
        
        if existing_vote.data:
            raise HTTPException(status_code=400, detail="You have already voted on this innovation")

        # Create vote record
        vote_record = {
            "innovation_id": str(innovation_id),
            "voter_identifier": voter_identifier,
            "vote_type": vote_data.vote_type.value,
            "comment": vote_data.comment,
            "user_agent": user_agent[:500] if user_agent else None,
            "ip_hash": hashlib.sha256(client_ip.encode()).hexdigest() if client_ip else None,
        }

        response = supabase.table("innovation_votes").insert(vote_record).execute()
        
        if not response.data:
            raise HTTPException(status_code=500, detail="Failed to record vote")

        created_vote = response.data[0]

        # Update innovation verification status in background
        from utils.voting_utils import update_innovation_verification_status
        await update_innovation_verification_status(innovation_id)

        return InnovationVoteResponse(
            id=created_vote["id"],
            innovation_id=created_vote["innovation_id"],
            vote_type=created_vote["vote_type"],
            comment=created_vote.get("comment"),
            created_at=created_vote["created_at"]
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error submitting innovation vote: {e}")
        raise HTTPException(status_code=500, detail="Failed to submit vote")


@app.get("/api/innovations/{innovation_id}/votes", response_model=InnovationVotingStats)
@limiter.limit("60/minute")
async def get_innovation_voting_stats(innovation_id: UUID, request: Request):
    """Get voting statistics for an innovation"""
    try:
        from config.database import get_supabase

        supabase = get_supabase()

        # Verify innovation exists
        innovation_response = supabase.table("innovations").select("id, verification_status").eq("id", str(innovation_id)).execute()
        if not innovation_response.data:
            raise HTTPException(status_code=404, detail="Innovation not found")

        # Get all votes for this innovation
        votes_response = supabase.table("innovation_votes").select("vote_type").eq("innovation_id", str(innovation_id)).execute()
        votes = votes_response.data if votes_response.data else []

        # Count votes by type
        yes_votes = len([v for v in votes if v["vote_type"] == "yes"])
        no_votes = len([v for v in votes if v["vote_type"] == "no"])
        need_more_info_votes = len([v for v in votes if v["vote_type"] == "need_more_info"])
        total_votes = len(votes)

        # Calculate confidence score for ML (simple algorithm for now)
        if total_votes == 0:
            confidence_score = 0.0
        else:
            # Confidence increases with more "yes" votes and total volume
            yes_ratio = yes_votes / total_votes
            volume_factor = min(1.0, total_votes / 10)  # Full confidence at 10+ votes
            confidence_score = yes_ratio * volume_factor

        # Determine verification status based on votes
        from utils.voting_utils import compute_verification_status
        current_status = innovation_response.data[0]["verification_status"]
        computed_status = compute_verification_status(yes_votes, no_votes, need_more_info_votes, total_votes)

        return InnovationVotingStats(
            innovation_id=innovation_id,
            total_votes=total_votes,
            yes_votes=yes_votes,
            no_votes=no_votes,
            need_more_info_votes=need_more_info_votes,
            confidence_score=round(confidence_score, 3),
            verification_status=computed_status
        )

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error getting innovation voting stats: {e}")
        raise HTTPException(status_code=500, detail="Failed to get voting stats")


# Statistics Endpoints
@app.get("/api/stats", response_model=InnovationStats)
@limiter.limit("10/minute")
async def get_statistics(request: Request):
    """Get platform statistics using Supabase client"""
    try:
        from config.database import get_supabase

        supabase = get_supabase()

        # Start with basic response structure
        result = {
            "total_innovations": 0,
            "verified_innovations": 0,
            "pending_innovations": 0,
            "innovations_by_type": {},
            "innovations_by_country": {},
            "innovations_by_month": {},
            "total_funding": None,
            "average_funding": None,
        }

        # Try to get basic count first
        try:
            total_response = (
                supabase.table("innovations").select("*", count="exact").execute()
            )
            result["total_innovations"] = (
                total_response.count if total_response.count is not None else 0
            )

            # If we have data, process it
            if total_response.data:
                verified_count = 0
                pending_count = 0
                type_counts = {}
                country_counts = {}

                for item in total_response.data:
                    # Count by verification status
                    status = item.get("verification_status", "unknown")
                    if status == "verified":
                        verified_count += 1
                    elif status == "pending":
                        pending_count += 1

                    # Count by type
                    innovation_type = item.get("innovation_type", "Unknown")
                    type_counts[innovation_type] = (
                        type_counts.get(innovation_type, 0) + 1
                    )

                    # Count by country
                    country = item.get("country", "Unknown")
                    country_counts[country] = country_counts.get(country, 0) + 1

                result["verified_innovations"] = verified_count
                result["pending_innovations"] = pending_count
                result["innovations_by_type"] = type_counts
                result["innovations_by_country"] = country_counts

        except Exception as query_error:
            logger.error(f"Error in statistics query: {query_error}")
            # Return basic structure even if query fails
            pass

        return result

    except Exception as e:
        logger.error(f"Error getting statistics: {e}")
        raise HTTPException(
            status_code=500, detail=f"Failed to get statistics: {str(e)}"
        )


# Background Task Functions
async def add_innovation_to_vector_db(
    innovation_id: UUID,
    title: str,
    description: str,
    innovation_type: str,
    country: str,
):
    """Add innovation to vector database"""
    try:
        vector_service = await get_vector_service()
        success = await vector_service.add_innovation(
            innovation_id=innovation_id,
            title=title,
            description=description,
            innovation_type=innovation_type,
            country=country,
        )

        if success:
            logger.info(f"Added innovation {innovation_id} to vector database")
        else:
            logger.error(f"Failed to add innovation {innovation_id} to vector database")

    except Exception as e:
        logger.error(f"Error adding innovation to vector DB: {e}")


async def run_academic_etl(job_id: str, days_back: int, max_results: int):
    """Run academic ETL job"""
    try:
        logger.info(f"Starting academic ETL job {job_id}")

        papers = await scrape_arxiv_papers(days_back, max_results)

        # Process and store papers (implementation would be more complex)
        logger.info(
            f"Academic ETL job {job_id} completed: {len(papers)} papers processed"
        )

    except Exception as e:
        logger.error(f"Academic ETL job {job_id} failed: {e}")


async def run_news_etl(job_id: str, hours_back: int):
    """Run news ETL job"""
    try:
        logger.info(f"Starting news ETL job {job_id}")

        articles = await monitor_rss_feeds(hours_back)

        # Process and store articles (implementation would be more complex)
        logger.info(
            f"News ETL job {job_id} completed: {len(articles)} articles processed"
        )

    except Exception as e:
        logger.error(f"News ETL job {job_id} failed: {e}")


async def run_serper_search(
    job_id: str,
    innovation_type: Optional[str],
    country: Optional[str],
    num_results: int,
):
    """Run Serper search job"""
    try:
        logger.info(f"Starting Serper search job {job_id}")

        results = await search_african_innovations(
            innovation_type, country, num_results
        )

        # Process and store results (implementation would be more complex)
        logger.info(
            f"Serper search job {job_id} completed: {len(results)} results processed"
        )

    except Exception as e:
        logger.error(f"Serper search job {job_id} failed: {e}")


# ETL Monitoring Endpoints
@app.get("/api/etl/status")
@limiter.limit("30/minute")
async def get_etl_status(request):
    """Get comprehensive ETL validation system status"""
    try:
        from services.etl_monitor import etl_monitor

        dashboard_data = etl_monitor.get_dashboard_data()
        return dashboard_data
    except Exception as e:
        logger.error(f"Error getting ETL status: {e}")
        raise HTTPException(status_code=500, detail="Failed to get ETL status")


@app.get("/api/etl/health")
@limiter.limit("60/minute")
async def get_etl_health(request):
    """Get ETL system health check"""
    try:
        from services.etl_monitor import etl_monitor

        health = etl_monitor.get_system_health()
        return {
            "status": "healthy" if health.database_status == "healthy" else "degraded",
            "system_health": {
                "cpu_percent": health.cpu_percent,
                "memory_percent": health.memory_percent,
                "disk_usage": health.disk_usage,
                "database_status": health.database_status,
                "vector_db_status": health.vector_db_status,
            },
            "last_check": health.last_check.isoformat(),
        }
    except Exception as e:
        logger.error(f"Error getting ETL health: {e}")
        raise HTTPException(status_code=500, detail="ETL health check failed")


@app.get("/api/validation/summary")
@limiter.limit("20/minute")
async def get_validation_summary(request: Request):
    """Get validation system summary for homepage using Supabase client"""
    try:
        from services.etl_monitor import etl_monitor

        summary = etl_monitor.get_validation_summary()

        # Add database counts using Supabase
        from config.database import get_supabase

        supabase = get_supabase()

        try:
            # Get total innovations count
            total_response = (
                supabase.table("innovations").select("id", count="exact").execute()
            )
            total_innovations = (
                total_response.count if total_response.count is not None else 0
            )

            # Get verified innovations count
            verified_response = (
                supabase.table("innovations")
                .select("id", count="exact")
                .eq("verification_status", "verified")
                .execute()
            )
            verified_innovations = (
                verified_response.count if verified_response.count is not None else 0
            )

            summary.update(
                {
                    "innovations_tracked": total_innovations,
                    "innovations_verified": verified_innovations,
                    "validation_rate": round(
                        (verified_innovations / max(1, total_innovations)) * 100, 1
                    ),
                }
            )
        except Exception as db_error:
            logger.error(
                f"Error getting database counts for validation summary: {db_error}"
            )
            # Continue with ETL summary even if database counts fail
            summary.update(
                {
                    "innovations_tracked": 0,
                    "innovations_verified": 0,
                    "validation_rate": 0.0,
                }
            )

        return summary

    except Exception as e:
        logger.error(f"Error getting validation summary: {e}")
        raise HTTPException(status_code=500, detail="Failed to get validation summary")


@app.get("/api/etl/jobs")
@limiter.limit("30/minute")
async def get_etl_jobs(request, active_only: bool = False):
    """Get ETL job statuses"""
    try:
        from services.etl_monitor import etl_monitor

        dashboard_data = etl_monitor.get_dashboard_data()

        jobs = dashboard_data["job_statuses"]
        if active_only:
            jobs = [
                job
                for job in jobs
                if job["is_running"] or job["health_status"] in ["healthy", "failing"]
            ]

        return {
            "jobs": jobs,
            "summary": dashboard_data["job_summary"],
            "timestamp": dashboard_data["timestamp"],
        }
    except Exception as e:
        logger.error(f"Error getting ETL jobs: {e}")
        raise HTTPException(status_code=500, detail="Failed to get ETL jobs")


@app.get("/api/etl/recent")
@limiter.limit("30/minute")
async def get_recent_etl_activity(request, hours: int = Query(24, ge=1, le=168)):
    """Get recent ETL activity"""
    try:
        from services.etl_monitor import etl_monitor

        activity = etl_monitor.get_recent_discoveries(hours)
        return {
            "activity": activity,
            "hours": hours,
            "timestamp": datetime.now().isoformat(),
        }
    except Exception as e:
        logger.error(f"Error getting recent ETL activity: {e}")
        raise HTTPException(status_code=500, detail="Failed to get ETL activity")


# Vector Database Monitoring Endpoints
@app.get("/api/vector/stats")
@limiter.limit("20/minute")
async def get_vector_stats(
    request: Request, vector_service: VectorService = Depends(get_vector_service)
):
    """Get vector database statistics and coverage"""
    try:
        # Get vector database stats
        vector_stats = await vector_service.get_stats()

        # Get database counts from Supabase
        from config.database import get_supabase

        supabase = get_supabase()

        # Count total innovations and publications
        innovations_response = (
            supabase.table("innovations").select("id", count="exact").execute()
        )
        publications_response = (
            supabase.table("publications").select("id", count="exact").execute()
        )

        total_innovations = (
            innovations_response.count if innovations_response.count is not None else 0
        )
        total_publications = (
            publications_response.count
            if publications_response.count is not None
            else 0
        )
        total_documents = total_innovations + total_publications

        # Calculate coverage (approximate)
        total_vectors = vector_stats.get("total_vectors", 0)
        coverage_percentage = (
            round((total_vectors / max(1, total_documents)) * 100, 1)
            if total_documents > 0
            else 0
        )

        return {
            "vector_database": vector_stats,
            "coverage": {
                "total_documents_in_db": total_documents,
                "innovations_in_db": total_innovations,
                "publications_in_db": total_publications,
                "total_vectors": total_vectors,
                "coverage_percentage": coverage_percentage,
                "estimated_missing": max(0, total_documents - total_vectors),
            },
            "status": "healthy" if total_vectors > 0 else "needs_rebuild",
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"Error getting vector statistics: {e}")
        raise HTTPException(status_code=500, detail="Failed to get vector statistics")


@app.get("/api/vector/search-quality")
@limiter.limit("30/minute")
async def test_search_quality(
    request: Request,
    test_queries: List[str] = Query(
        ["AI agriculture", "machine learning healthcare", "fintech innovation"]
    ),
    vector_service: VectorService = Depends(get_vector_service),
):
    """Test search quality with sample queries"""
    try:
        results = {}

        for query in test_queries[:5]:  # Limit to 5 queries
            try:
                # Test vector search
                vector_results = await vector_service.search_innovations(query, top_k=5)

                # Calculate quality metrics
                avg_score = (
                    sum(r.score for r in vector_results) / len(vector_results)
                    if vector_results
                    else 0
                )

                results[query] = {
                    "results_count": len(vector_results),
                    "avg_relevance_score": round(avg_score, 3),
                    "top_results": [
                        {
                            "title": r.metadata.get("title", "No title"),
                            "score": round(r.score, 3),
                            "type": r.metadata.get("innovation_type", "Unknown"),
                        }
                        for r in vector_results[:3]
                    ],
                }
            except Exception as query_error:
                results[query] = {
                    "error": str(query_error),
                    "results_count": 0,
                    "avg_relevance_score": 0,
                }

        # Calculate overall quality
        total_results = sum(r.get("results_count", 0) for r in results.values())
        avg_quality = sum(
            r.get("avg_relevance_score", 0) for r in results.values()
        ) / len(results)

        return {
            "test_results": results,
            "overall_quality": {
                "total_test_queries": len(test_queries),
                "total_results_found": total_results,
                "average_relevance_score": round(avg_quality, 3),
                "quality_rating": "excellent"
                if avg_quality > 0.8
                else "good"
                if avg_quality > 0.6
                else "needs_improvement",
            },
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"Error testing search quality: {e}")
        raise HTTPException(status_code=500, detail="Failed to test search quality")


# AI Backfill Endpoints
@app.post("/api/ai-backfill/trigger")
@limiter.limit("5/minute")
async def trigger_ai_backfill(
    request: Request,
    background_tasks: BackgroundTasks,
    innovation_ids: Optional[List[str]] = None,
    max_jobs: int = Query(
        10, ge=1, le=50, description="Maximum number of jobs to process"
    ),
):
    """Manually trigger AI backfill for specific innovations or all pending"""
    try:
        job_id = str(uuid4())

        # Add background task
        background_tasks.add_task(run_ai_backfill_job, job_id, innovation_ids, max_jobs)

        logger.info(f"AI backfill triggered: job_id={job_id}, max_jobs={max_jobs}")

        return {
            "job_id": job_id,
            "status": "started",
            "message": f"AI backfill started for up to {max_jobs} innovations",
            "innovation_ids": innovation_ids,
        }

    except Exception as e:
        logger.error(f"Error triggering AI backfill: {e}")
        raise HTTPException(status_code=500, detail="Failed to trigger AI backfill")


@app.post("/api/ai-backfill/innovation/{innovation_id}")
@limiter.limit("10/minute")
async def backfill_single_innovation(
    innovation_id: str, request: Request, background_tasks: BackgroundTasks
):
    """Trigger AI backfill for a specific innovation"""
    try:
        # Get innovation from database
        from config.database import get_supabase

        supabase = get_supabase()

        response = (
            supabase.table("innovations").select("*").eq("id", innovation_id).execute()
        )

        if not response.data:
            raise HTTPException(status_code=404, detail="Innovation not found")

        innovation = response.data[0]
        job_id = str(uuid4())

        # Add background task for single innovation
        background_tasks.add_task(run_single_innovation_backfill, job_id, innovation)

        logger.info(
            f"Single innovation backfill triggered: job_id={job_id}, innovation_id={innovation_id}"
        )

        return {
            "job_id": job_id,
            "status": "started",
            "innovation_id": innovation_id,
            "innovation_title": innovation.get("title"),
            "message": "AI backfill started for innovation",
        }

    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Error triggering single innovation backfill: {e}")
        raise HTTPException(
            status_code=500, detail="Failed to trigger innovation backfill"
        )


@app.get("/api/ai-backfill/status")
@limiter.limit("30/minute")
async def get_backfill_status(request: Request):
    """Get current AI backfill system status"""
    try:
        from services.integration_guide import get_ai_backfill_system_status

        status = get_ai_backfill_system_status()

        return {
            "status": "operational",
            "backfill_system": status,
            "timestamp": datetime.now().isoformat(),
        }

    except Exception as e:
        logger.error(f"Error getting backfill status: {e}")
        raise HTTPException(status_code=500, detail="Failed to get backfill status")


@app.get("/api/ai-backfill/stats")
@limiter.limit("20/minute")
async def get_backfill_stats(request: Request):
    """Get AI backfill service statistics"""
    try:
        stats = ai_backfill_service.get_backfill_stats()

        return {"service_stats": stats, "timestamp": datetime.now().isoformat()}

    except Exception as e:
        logger.error(f"Error getting backfill stats: {e}")
        raise HTTPException(status_code=500, detail="Failed to get backfill stats")


@app.post("/api/ai-backfill/schedule")
@limiter.limit("2/minute")
async def update_backfill_schedule(
    request: Request,
    interval_hours: int = Query(
        3, ge=1, le=24, description="Backfill interval in hours"
    ),
):
    """Update the AI backfill schedule interval"""
    try:
        from datetime import timedelta

        backfill_integration.backfill_interval = timedelta(hours=interval_hours)

        logger.info(f"Updated AI backfill interval to {interval_hours} hours")

        return {
            "status": "updated",
            "interval_hours": interval_hours,
            "message": f"AI backfill will now run every {interval_hours} hours",
        }

    except Exception as e:
        logger.error(f"Error updating backfill schedule: {e}")
        raise HTTPException(
            status_code=500, detail="Failed to update backfill schedule"
        )


@app.post("/api/vector/rebuild")
@limiter.limit("2/hour")  # Very limited - this is an expensive operation
async def trigger_vector_rebuild(
    request: Request,
    background_tasks: BackgroundTasks,
    document_types: List[str] = Query(["innovations", "publications"]),
):
    """Trigger vector database rebuild (admin only)"""
    try:
        job_id = str(uuid4())

        # Add rebuild task to background
        background_tasks.add_task(run_vector_rebuild, job_id, document_types)

        return {
            "job_id": job_id,
            "status": "started",
            "message": f"Vector rebuild started for: {', '.join(document_types)}",
            "estimated_duration": "5-15 minutes",
            "warning": "This will regenerate all embeddings and may affect search temporarily",
        }

    except Exception as e:
        logger.error(f"Error triggering vector rebuild: {e}")
        raise HTTPException(status_code=500, detail="Failed to trigger vector rebuild")


async def run_ai_backfill_job(
    job_id: str, innovation_ids: Optional[List[str]], max_jobs: int
):
    """Run AI backfill job in background"""
    try:
        logger.info(f"Starting AI backfill job {job_id}, max_jobs={max_jobs}")

        if innovation_ids:
            # Backfill specific innovations
            from config.database import get_supabase

            supabase = get_supabase()

            innovations = []
            for innovation_id in innovation_ids:
                response = (
                    supabase.table("innovations")
                    .select("*")
                    .eq("id", innovation_id)
                    .execute()
                )
                if response.data:
                    innovations.extend(response.data)

            if innovations:
                jobs = await create_backfill_jobs_for_innovations(innovations)
                processed_jobs = await ai_backfill_service.run_scheduled_backfill(
                    max_jobs=min(max_jobs, len(jobs))
                )

                # Apply results to database
                await apply_backfill_results_to_database(processed_jobs)

                logger.info(
                    f"AI backfill job {job_id} completed: {len(processed_jobs)} jobs processed"
                )
            else:
                logger.warning(
                    f"AI backfill job {job_id}: No innovations found for specified IDs"
                )
        else:
            # Run scheduled backfill
            result = await backfill_integration.run_scheduled_backfill()
            logger.info(f"AI backfill job {job_id} completed: {result}")

    except Exception as e:
        logger.error(f"AI backfill job {job_id} failed: {e}")


async def run_single_innovation_backfill(job_id: str, innovation: dict):
    """Run AI backfill for a single innovation"""
    try:
        logger.info(
            f"Starting single innovation backfill job {job_id} for {innovation.get('title')}"
        )

        # Create backfill job for single innovation
        job = await ai_backfill_service.create_backfill_job(innovation)

        if job:
            # Process the job
            processed_job = await ai_backfill_service.process_backfill_job(job)

            # Apply results to database
            if processed_job.status.value == "completed":
                await apply_single_backfill_result_to_database(processed_job)
                logger.info(
                    f"Single innovation backfill job {job_id} completed successfully"
                )
            else:
                logger.warning(
                    f"Single innovation backfill job {job_id} failed: {processed_job.error_message}"
                )
        else:
            logger.info(
                f"Single innovation backfill job {job_id}: No backfill needed for {innovation.get('title')}"
            )

    except Exception as e:
        logger.error(f"Single innovation backfill job {job_id} failed: {e}")


async def apply_backfill_results_to_database(processed_jobs):
    """Apply multiple backfill results to database"""
    from config.database import get_supabase

    supabase = get_supabase()

    for job in processed_jobs:
        if job.status.value == "completed" and job.results:
            try:
                updates = await create_database_updates_from_backfill_job(job)
                if updates:
                    response = (
                        supabase.table("innovations")
                        .update(updates)
                        .eq("id", job.innovation_id)
                        .execute()
                    )
                    if response.data:
                        logger.info(
                            f"Applied backfill updates for innovation {job.innovation_id}"
                        )
            except Exception as e:
                logger.error(f"Error applying backfill for {job.innovation_id}: {e}")


async def apply_single_backfill_result_to_database(job):
    """Apply single backfill result to database"""
    from config.database import get_supabase

    supabase = get_supabase()

    try:
        updates = await create_database_updates_from_backfill_job(job)
        if updates:
            response = (
                supabase.table("innovations")
                .update(updates)
                .eq("id", job.innovation_id)
                .execute()
            )
            if response.data:
                logger.info(
                    f"Applied single backfill updates for innovation {job.innovation_id}"
                )
    except Exception as e:
        logger.error(f"Error applying single backfill for {job.innovation_id}: {e}")


async def create_database_updates_from_backfill_job(job) -> dict:
    """Convert backfill job results to database update format"""
    updates = {}

    for field_name, result in job.results.items():
        if isinstance(result, dict) and "error" not in result:
            confidence = result.get("confidence_score", 0.0)
            if confidence < 0.6:
                continue

            value = result.get("new_value")
            if not value:
                continue

            # Map field names to database columns
            if field_name == "funding_amount" and isinstance(value, dict):
                # Add to existing fundings array or create new one
                funding_data = {
                    "amount": value.get("amount"),
                    "currency": value.get("currency", "USD"),
                    "funding_type": "ai_backfilled",
                    "funder_name": "AI Discovered",
                    "verified": False,
                }
                updates["fundings"] = [funding_data]

            elif field_name == "website_url":
                updates["website_url"] = value

            elif field_name == "github_url":
                updates["github_url"] = value

            elif field_name == "demo_url":
                updates["demo_url"] = value

            elif field_name == "key_team_members" and isinstance(value, dict):
                team_data = []
                for member in value.get("team_members", []):
                    team_data.append(
                        {"name": member, "role": "ai_discovered", "country": None}
                    )
                updates["individuals"] = team_data

            elif field_name == "user_metrics" and isinstance(value, dict):
                updates["impact_metrics"] = value

    # Add metadata about the AI backfill
    if updates:
        updates["ai_backfill_metadata"] = {
            "last_backfill": datetime.now().isoformat(),
            "backfill_version": "1.0",
            "requires_human_review": True,
        }

    return updates


async def run_vector_rebuild(job_id: str, document_types: List[str]):
    """Run vector database rebuild in background"""
    try:
        logger.info(f"Starting vector rebuild job {job_id} for types: {document_types}")

        # Import the rebuilder
        from scripts.rebuild_vectors import VectorRebuilder

        rebuilder = VectorRebuilder()

        if not await rebuilder.initialize():
            logger.error(f"Vector rebuild job {job_id} failed to initialize")
            return

        success = True

        if "innovations" in document_types:
            if not await rebuilder.rebuild_innovations():
                success = False

        if "publications" in document_types:
            if not await rebuilder.rebuild_publications():
                success = False

        await rebuilder.save_stats()

        if success:
            logger.info(f"Vector rebuild job {job_id} completed successfully")
        else:
            logger.warning(f"Vector rebuild job {job_id} completed with errors")

    except Exception as e:
        logger.error(f"Vector rebuild job {job_id} failed: {e}")


if __name__ == "__main__":
    import uvicorn

    # Use PORT from environment, settings, or default to 8030 for production compatibility
    port = int(os.getenv("PORT", settings.PORT))

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=settings.DEBUG,
        log_level=settings.LOG_LEVEL.lower(),
    )
